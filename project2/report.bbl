% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{LunarLander}
``Lunarlander,'' \url{https://gym.openai.com/envs/LunarLander-v2/}.

\bibitem{opengym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba, ``Openai gym,'' 2016.

\bibitem{mataric1994reward}
M.~J. Mataric, ``Reward functions for accelerated learning,'' in \emph{Machine
  Learning Proceedings 1994}.\hskip 1em plus 0.5em minus 0.4em\relax Elsevier,
  1994, pp. 181--189.

\bibitem{ng1999policy}
A.~Y. Ng, D.~Harada, and S.~Russell, ``Policy invariance under reward
  transformations: Theory and application to reward shaping,'' in \emph{ICML},
  vol.~99, 1999, pp. 278--287.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski \emph{et~al.},
  ``Human-level control through deep reinforcement learning,'' \emph{Nature},
  vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{lin1992self}
L.-J. Lin, ``Self-improving reactive agents based on reinforcement learning,
  planning and teaching,'' \emph{Machine learning}, vol.~8, no. 3-4, pp.
  293--321, 1992.

\bibitem{hasselt2010double}
H.~V. Hasselt, ``Double q-learning,'' in \emph{Advances in neural information
  processing systems}, 2010, pp. 2613--2621.

\bibitem{HasseltGS15DDQL}
\BIBentryALTinterwordspacing
H.~van Hasselt, A.~Guez, and D.~Silver, ``Deep reinforcement learning with
  double q-learning,'' \emph{CoRR}, vol. abs/1509.06461, 2015. [Online].
  Available: \url{http://arxiv.org/abs/1509.06461}
\BIBentrySTDinterwordspacing

\bibitem{schaul2015prioritized}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver, ``Prioritized experience
  replay,'' \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem{wang2015dueling}
Z.~Wang, T.~Schaul, M.~Hessel, H.~Van~Hasselt, M.~Lanctot, and N.~De~Freitas,
  ``Dueling network architectures for deep reinforcement learning,''
  \emph{arXiv preprint arXiv:1511.06581}, 2015.

\bibitem{ddqn}
``Double deep q networks--tackling maximization bias in deep q-learning,''
  \url{https://towardsdatascience.com/double-deep-q-networks-905dd8325412}.

\bibitem{ddqn2}
``Self learning ai-agents iii:deep (double) q-learning,''
  \url{https://towardsdatascience.com/deep-double-q-learning-7fca410b193a}.

\bibitem{ddqn3}
``Deep q-learning, part2: Double deep q network, (double dqn),''
  \url{https://medium.com/@qempsil0914/deep-q-learning-part2-double-deep-q-network-double-dqn-b8fc9212bbb2}.

\bibitem{improvedqn}
``Improvements in deep q learning: Dueling double dqn, prioritized experience
  replay, and fixedâ€¦),''
  \url{https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/}.

\bibitem{duelingDQN}
``Dueling deep q networks,''
  \url{https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751}.

\end{thebibliography}
