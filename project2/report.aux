\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}introduction to LunarLander}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Double Deep Q-Learning}{1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces $\epsilon $-greedy}}{1}}
\newlabel{algo:seq}{{1}{1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Double Deep-Q Learning}}{2}}
\newlabel{algo:seq}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experiment Results}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Training Agent}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Effect of Hyperparameters}{2}}
\bibstyle{IEEEtran}
\bibdata{reference}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Effect of learning rate. Episode rewards of 1000 episodes for different $decay$ values during training process. $decay=0.995$, $\tau =0.001$ for all runs..}}{3}}
\newlabel{fig:lr}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Effect of epsilon\_decay. Episode rewards of 1000 episodes for different $decay$ values during training process. $\tau =0.001$, $lr=0.0005$ for all runs.}}{3}}
\newlabel{fig:epsilon_decay}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}conclusion}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Effect of $\tau $. Episode rewards of 1000 episodes for different $\tau $ values during training process. $decay=0.995$, $lr=0.0005$ for all runs.}}{3}}
\newlabel{fig:tau}{{3}{3}}
