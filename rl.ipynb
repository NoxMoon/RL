{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class game():\n",
    "    def __init__(self, c_, r_, R_):\n",
    "        self.s = (c_, r_)\n",
    "        self.nrow = 3\n",
    "        self.ncol = 4\n",
    "        \n",
    "        assert 0<=self.s[0]<self.ncol and 0<=self.s[1]<self.nrow, \"invalid initial position\"\n",
    "        self.R = {}\n",
    "        self.T = defaultdict(lambda: defaultdict(int))\n",
    "        self.actions = {'up':(1,0),'down':(-1,0),'left':(0,-1),'right':(0,1)}\n",
    "        \n",
    "        self.states = set([(0,0),(0,1),(0,2),(0,3),(1,0),(1,2),(1,3),(2,0),(2,1),(2,2),(2,3),'end'])\n",
    "        for s in self.states:\n",
    "            self.R[s] = R_\n",
    "        self.R[(1,3)] = -1\n",
    "        self.R[(2,3)] = 1\n",
    "        self.R['end'] = 0\n",
    "        \n",
    "        for s in self.states:\n",
    "            if s == (1,3) or s == (2,3) or s=='end':\n",
    "                for a in self.actions:\n",
    "                    self.T[s,a]['end'] = 1\n",
    "                continue\n",
    "            x1, y1 = s\n",
    "            for a, (dx,dy) in self.actions.items():\n",
    "                x2, y2 = x1 + dx, y1 + dy\n",
    "                if (x2,y2) in self.states:\n",
    "                    self.T[(x1,y1),a][(x2,y2)] += 0.8\n",
    "                else:\n",
    "                    self.T[(x1,y1),a][(x1,y1)] += 0.8\n",
    "                x2, y2 = x1 + dy, y1 + dx\n",
    "                if (x2,y2) in self.states:\n",
    "                    self.T[(x1,y1),a][(x2,y2)] += 0.1\n",
    "                else:\n",
    "                    self.T[(x1,y1),a][(x1,y1)] += 0.1\n",
    "                x2, y2 = x1 - dy, y1 - dx\n",
    "                if (x2,y2) in self.states:\n",
    "                    self.T[(x1,y1),a][(x2,y2)] += 0.1\n",
    "                else:\n",
    "                    self.T[(x1,y1),a][(x1,y1)] += 0.1\n",
    "                    \n",
    "    def move(self, action):\n",
    "        R = self.R[self.s]\n",
    "        \n",
    "        next_s = list(self.T[self.s, action].keys())\n",
    "        p = list(self.T[self.s, action].values())\n",
    "        #print(next_s, p)\n",
    "        i = np.random.choice(len(next_s), p=p)\n",
    "        self.s = next_s[i]\n",
    "        \n",
    "        return R\n",
    "    \n",
    "    def reset_pos(self):\n",
    "        self.s = list(self.states)[np.random.randint(len(self.states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(game, max_iteration=1000, beta=1):\n",
    "    # initialize\n",
    "    V = {s:0 for s in game.states}\n",
    "    \n",
    "    for n in range(max_iteration):\n",
    "        V_new = {}\n",
    "        for s in game.states:\n",
    "            V_new[s] = game.R[s] + beta * max(sum(T*V[s_next] for s_next, T in game.T[(s,a)].items()) for a in game.actions)\n",
    "        #print(V_new)\n",
    "        if all(abs(V_new[s]-V[s])<1e-8 for s in game.states):\n",
    "            print(f\"value iteration converged at {n}\")\n",
    "            break\n",
    "        V = V_new\n",
    "        \n",
    "    p = {}\n",
    "    for s in game.states:\n",
    "        q = {game.R[s] + beta * sum(T*V[s_next] for s_next, T in game.T[(s,a)].items()): a for a in game.actions}\n",
    "        #print(s, q)\n",
    "        p[s] = q[max(q.keys())]\n",
    "    return V, p\n",
    "\n",
    "def policy_iteration(game, max_iteration=1000, beta=1):\n",
    "    \n",
    "    p = {s:np.random.choice(list(grid_game.actions.keys())) for s in game.states}\n",
    "    V = {s:0 for s in game.states}\n",
    "    \n",
    "    for n in range(max_iteration):\n",
    "        for m in range(max_iteration):\n",
    "            V_new = {}\n",
    "            for s in game.states:\n",
    "                V_new[s] = game.R[s] + beta * sum(T*V[s_next] for s_next, T in game.T[s, p[s]].items())\n",
    "            if all(abs(V_new[s]-V[s])<1e-8 for s in game.states):\n",
    "                break\n",
    "            V = V_new\n",
    "        p_new = {}\n",
    "        for s in game.states:\n",
    "            q = {game.R[s] + beta * sum(T*V[s_next] for s_next, T in game.T[(s,a)].items()): a for a in game.actions}\n",
    "            p_new[s] = q[max(q.keys())]\n",
    "        if all(p_new[s]==p[s] for s in game.states):\n",
    "            print(f\"policy iteration converged at {n}\")\n",
    "            break\n",
    "        p = p_new\n",
    "        \n",
    "    return V, p\n",
    "\n",
    "\n",
    "def Q_learning(game, max_iteration=10000, alpha=0.8, random_explore=0.05, beta=1):\n",
    "    \n",
    "    Q = {s:{a:0 for a in game.actions} for s in game.states}\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        game.reset_pos()\n",
    "        tmpQ = {s:{a:Q[s][a] for a in game.actions} for s in game.states}\n",
    "        while game.s != 'end':\n",
    "            curr_s = game.s\n",
    "            q = {q: a for a,q in Q[curr_s].items()}\n",
    "            next_a = q[max(q.keys())]\n",
    "            if np.random.random()<random_explore:\n",
    "                next_a = np.random.choice(list(grid_game.actions.keys()))\n",
    "            R = game.move(next_a)\n",
    "            Q[curr_s][next_a] = (1-alpha) * Q[curr_s][next_a] + alpha * (R + beta * max(Q[game.s].values()))\n",
    "#         if i%1000==0:\n",
    "#             print(i, sum(abs(Q[s][a]-tmpQ[s][a]) for s in Q for a in Q[s]))\n",
    "            \n",
    "    p = {}\n",
    "    V = {}\n",
    "    for s in game.states:\n",
    "        q = {Q[s][a]: a for a in game.actions}\n",
    "        p[s] = q[max(q.keys())]\n",
    "        V[s] = max(Q[s].values())\n",
    "    return V, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8320000000000001\n",
      "1000 1.1471591471570615\n",
      "2000 0.0\n",
      "3000 0.13473277836906494\n",
      "4000 0.05457824747646567\n",
      "5000 0.0\n",
      "6000 0.0\n",
      "7000 0.0\n",
      "8000 0.0\n",
      "9000 0.1292776620297249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 1): 0.7559494871520556,\n",
       "  (1, 2): 0.8448496478933267,\n",
       "  (0, 0): 0.7886263247458721,\n",
       "  (1, 3): -1.0,\n",
       "  'end': 0,\n",
       "  (2, 1): 0.9187199536992825,\n",
       "  (2, 0): 0.8748266443254658,\n",
       "  (2, 3): 1.0,\n",
       "  (2, 2): 0.9462951710209209,\n",
       "  (1, 0): 0.8345397682031598,\n",
       "  (0, 2): 0.68102900008528,\n",
       "  (0, 3): 0.28483994826770426},\n",
       " {(0, 1): 'left',\n",
       "  (1, 2): 'left',\n",
       "  (0, 0): 'up',\n",
       "  (1, 3): 'right',\n",
       "  'end': 'right',\n",
       "  (2, 1): 'right',\n",
       "  (2, 0): 'right',\n",
       "  (2, 3): 'right',\n",
       "  (2, 2): 'right',\n",
       "  (1, 0): 'up',\n",
       "  (0, 2): 'left',\n",
       "  (0, 3): 'left'})"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_game = game(0,0,-0.04)\n",
    "Q_learning(grid_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value iteration converged at 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 1): 0.6553082174525494,\n",
       "  (1, 2): 0.6602739726027294,\n",
       "  (0, 0): 0.7053082186269344,\n",
       "  (1, 3): -1,\n",
       "  'end': 0,\n",
       "  (2, 1): 0.867808219178066,\n",
       "  (2, 0): 0.8115582191696421,\n",
       "  (2, 3): 1,\n",
       "  (2, 2): 0.9178082191780785,\n",
       "  (1, 0): 0.7615582191497039,\n",
       "  (0, 2): 0.6114155212421983,\n",
       "  (0, 3): 0.38792490255093054},\n",
       " {(0, 1): 'left',\n",
       "  (1, 2): 'up',\n",
       "  (0, 0): 'up',\n",
       "  (1, 3): 'right',\n",
       "  'end': 'right',\n",
       "  (2, 1): 'right',\n",
       "  (2, 0): 'right',\n",
       "  (2, 3): 'right',\n",
       "  (2, 2): 'right',\n",
       "  (1, 0): 'up',\n",
       "  (0, 2): 'left',\n",
       "  (0, 3): 'left'})"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(grid_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration converged at 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 1): 0.6553082191745441,\n",
       "  (1, 2): 0.6602739726027398,\n",
       "  (0, 0): 0.7053082191769686,\n",
       "  (1, 3): -1,\n",
       "  'end': 0,\n",
       "  (2, 1): 0.8678082191780823,\n",
       "  (2, 0): 0.8115582191780686,\n",
       "  (2, 3): 1,\n",
       "  (2, 2): 0.9178082191780822,\n",
       "  (1, 0): 0.7615582191780359,\n",
       "  (0, 2): 0.6114155250915418,\n",
       "  (0, 3): 0.3879249099008332},\n",
       " {(0, 1): 'left',\n",
       "  (1, 2): 'up',\n",
       "  (0, 0): 'up',\n",
       "  (1, 3): 'right',\n",
       "  'end': 'right',\n",
       "  (2, 1): 'right',\n",
       "  (2, 0): 'right',\n",
       "  (2, 3): 'right',\n",
       "  (2, 2): 'right',\n",
       "  (1, 0): 'up',\n",
       "  (0, 2): 'left',\n",
       "  (0, 3): 'left'})"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration(grid_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
