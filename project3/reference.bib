@misc{LunarLander,
	title = {LunarLander},
	howpublished = {\url{https://gym.openai.com/envs/LunarLander-v2/}},
}

@misc{opengym,
	Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	Title = {OpenAI Gym},
	Year = {2016},
	Eprint = {arXiv:1606.01540},
}

@incollection{mataric1994reward,
	title={Reward functions for accelerated learning},
	author={Mataric, Maja J},
	booktitle={Machine Learning Proceedings 1994},
	pages={181--189},
	year={1994},
	publisher={Elsevier}
}

@inproceedings{ng1999policy,
	title={Policy invariance under reward transformations: Theory and application to reward shaping},
	author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
	booktitle={ICML},
	volume={99},
	pages={278--287},
	year={1999}
} 

@article{HasseltGS15DDQL,
	author    = {Hado van Hasselt and
	Arthur Guez and
	David Silver},
	title     = {Deep Reinforcement Learning with Double Q-learning},
	journal   = {CoRR},
	volume    = {abs/1509.06461},
	year      = {2015},
	url       = {http://arxiv.org/abs/1509.06461},
	archivePrefix = {arXiv},
	eprint    = {1509.06461},
	timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HasseltGS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mnih2015human,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	journal={Nature},
	volume={518},
	number={7540},
	pages={529--533},
	year={2015},
	publisher={Nature Publishing Group}
}


@inproceedings{hasselt2010double,
	title={Double Q-learning},
	author={Hasselt, Hado V},
	booktitle={Advances in neural information processing systems},
	pages={2613--2621},
	year={2010}
}

@article{lin1992self,
	title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
	author={Lin, Long-Ji},
	journal={Machine learning},
	volume={8},
	number={3-4},
	pages={293--321},
	year={1992},
	publisher={Springer}
}

@article{schaul2015prioritized,
	title={Prioritized experience replay},
	author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	journal={arXiv preprint arXiv:1511.05952},
	year={2015}
}

@article{wang2015dueling,
	title={Dueling network architectures for deep reinforcement learning},
	author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Van Hasselt, Hado and Lanctot, Marc and De Freitas, Nando},
	journal={arXiv preprint arXiv:1511.06581},
	year={2015}
}

@misc{ddqn,
	title = {Double Deep Q Networks--Tackling maximization bias in Deep Q-learning},
	howpublished = {\url{https://towardsdatascience.com/double-deep-q-networks-905dd8325412}},
}

@misc{ddqn2,
	title = {Self Learning AI-Agents III:Deep (Double) Q-Learning},
	howpublished = {\url{https://towardsdatascience.com/deep-double-q-learning-7fca410b193a}},
}

@misc{ddqn3,
	title = {Deep Q-Learning, Part2: Double Deep Q Network, (Double DQN)},
	howpublished = {\url{https://medium.com/@qempsil0914/deep-q-learning-part2-double-deep-q-network-double-dqn-b8fc9212bbb2}},
}

@misc{improvedqn,
	title = {Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixedâ€¦)},
	howpublished = {\url{https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/}},
}


@misc{duelingDQN,
	title = {Dueling Deep Q Networks},
	howpublished = {\url{https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751}},
}