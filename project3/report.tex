%\documentclass{article}
%\documentclass[aip,jcp,amsmath,amssymb,reprint]{revtex4-1}

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\usepackage[utf8]{inputenc}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\minuseq}{\mathrel{-}=}
\newcommand{\asteq}{\mathrel{*}=}
\renewcommand{\algorithmiccomment}[1]{\hfill$\triangleright$\textit{#1}}

\title{CS7642 Project 3 -- Solving Soccer Game with Q-Learning, Friend/Foe-Q and Correlated-Q}
\author{Qinghui Ge}
\date{January 2020}

\begin{document}
	
	\maketitle
	
\begin{abstract}
In this project, we try to reproduce Figure 3 of Amy Greenward's paper, \textit{Correlated Q-learning}, 2003\cite{greenwald2003correlated}.  Correlated Q-learning is an algorithm tries to find the correlated equilibrium in Markov games. Correlated-Q generalize Nash-Q and Friend/Foe-Q, and can be solved with linear programming. In general sum games, correlated equilibria contains all Nash equilibria, while in constant sum games correlated equilibria contains minimax equilibria. We implemented Correlated-Q, as well as regular Q-learning, Friend-Q, Foe-Q and compare their behavior in solving soccer game --- a zero sum game. We show that in this particular game, Correlated-Q reaches the save solution as Foe-Q.

	git hash: 
\end{abstract}
	
	
\section{Soccer Game}
The soccer game is a two-player zero-sum game. On a $4\times2$ grid, there are two players A, B and a ball. For each player, the objective is to carry the ball to his or her designated goal area and receive +100 reward. Carrying the ball to opponent's area or allow the opponent to score would receive -100 reward. There are five possible actions for each player: N, S, E, W and Stick. Fig.\ref{fig:state_s} shows state $s$ (also the initial state of simulation) of soccer game.

\begin{figure}[!h]
	\centering
	\includegraphics[width=3.4in]{figures/state_s}
	\caption{Soccer game, state $s$ (Figure 4 or Greenward 2003 paper\cite{greenwald2003correlated})}
	\label{fig:state_s}
\end{figure}

There is certain ambiguity in the game rules described in Greenward's paper\cite{greenwald2003correlated}. We implement the rule as following: First determine which player (A or B) moves first, then execute two players' actions based on the random order. For each action execution, if the moving player's next position do not collide with the other player's current position, he can move to the new position, otherwise, he losses ball to the other player (if he has it). After executing two actions, check if the termination criterion is met and assign rewards. The game is implemented like an Opengym environment, with the ``step"-like function defined in Alg.\ref{algo:soccer}. The \textit{step} function takes a tuple of action $\mathbf{a}$=$(a_A,a_B)$, and returns the state index (based on two players position and ball ownership), reward as a tuple $\mathbf{R}$=$(R_A,R_B)$, and boolean variable \textit{done} to indicator whether the game has ended. For the soccer game, there are $8\times7\times2=112$ states.

\begin{algorithm}[h!]
	\caption{soccer game environment implementation}
	\begin{algorithmic}
		\Function{env.step}{$\mathbf{a}$=$(a_A,a_B)$}
		\State determine player move order
		\State get first player proposed move
		\If {no collision}:
			\State first player move
		\ElsIf {first player has ball}
			\State pass ball to second player
		\EndIf
		\State get second player proposed move
		\If {no collision}:
		\State second player move
		\ElsIf {second player has ball}
		\State pass ball to first player
		\EndIf
		\If {ball in player A's goal area}: 		\Comment check game status
			\State $\mathbf{R}$, done = (100,-100), True
		\ElsIf {ball in player B's goal area}:
			\State $\mathbf{R}$, done = (-100,100), True
		\Else
			\State $\mathbf{R}$, done = (0,0), False
		\EndIf
		\State Return s($pos_A$, $pos_B$, ball), $\mathbf{R}$, done
		\EndFunction
	\end{algorithmic}
	\label{algo:soccer}
\end{algorithm}

\section{Learner implementation}
We follow a general off-policy learning template (Alg. \ref{algo:learning}) for all training process, each learner (Q, Friend-Q, Foe-Q, CE-Q) has to implement it's own way of action selection (for $\epsilon$-greedy to call if greedy action is needed), and the function to update Q values. The general form of updating $Q$ values of agent $i$ is:

\begin{equation}
Q_i(s, \mathbf{a}) = (1-\alpha) Q_i(s, \mathbf{a}) + \alpha  \big( (1-\gamma) R_i + \gamma V_i(s') \big)
\label{eq:update}
\end{equation}

where $\alpha$ is learning rate, $\gamma$ is discount factor. $\mathbf{a}=(a_1, a_2\dots a_N)$ is a combination of all agent actions (although we will see in Q-Learning each agent only need to track the Q-table of its own action, while for other learners each agent also keep track of other agent's actions). Each learner defines state value function $V_i(s)$ differently.

Our implementation of Foe-Q is currently restricted to only two agents, while Q-Learning, Friend-Q, and CE-Q are generalized to handle multi-agents, with each agent potentially having different action choices. The joint action space of all agent is $\mathbf{A}=\{A_1, A_2 \dots A_N\}$. As Littman has point-out, Friend/Foe-Q can be combined into the same frame framework, with some agents as friend, and some agents as foe\cite{littman2001friend}.

In Greenward's 2003 paper\cite{greenwald2003correlated}, it is unclear how learning rate is decayed, and whether Friend/Foe-Q, CE-Q used $\epsilon$-greedy for action exploration. In Greenward's 2005 extented paper\cite{greenwald2005correlated}, the authors decay learning rate according to the number of times each state-action pair is visited, i.e. $\alpha(s,\mathbf{a}) = 1/n(s,\mathbf{a})$. The 2005 paper also employs $\epsilon=1$ (random action selection) for Friend/Foe-Q, CE-Q. We implemented this procedure described in Greenward's 2005 paper, as well as other possibilities such as decaying $\epsilon$ and $\alpha$ exponentially.

\begin{algorithm}[h!]
	\caption{off\_policy\_learning template}
	\begin{algorithmic}
		\Function{off\_policy\_learning}{learner, env}
			\For {each episode}
				\State reset environment, get current state $s$
				\For {t in episode}
					\State $\mathbf{a} = \epsilon$-greedy(learner, env, s, $\epsilon$)
					\State take action $\mathbf{a}$, observe $s'$, $\mathbf{R}$, done
					\State learner.update($s$, $s'$, $\mathbf{a}$, $\mathbf{R}$ done, $\alpha(s,\mathbf{a})$, $\gamma$)
				    \State decay $\alpha$, e.g. $\alpha(s,\mathbf{a}) = 1/n(s,\mathbf{a})$
				    \State optional: decay $\epsilon$, e.g. $\epsilon \asteq \epsilon\_decay$
				    \State $s=s'$
				\EndFor			
			\EndFor
		\EndFunction
	\end{algorithmic}
	\label{algo:learning}
\end{algorithm}

\subsection{Q-Learning}
In Q-Learning implementation, each agent $i$ maintains its own Q-table of size $nS\times nA_i$. The greedy action is each agent taking it's own greedy action, i.e:
\begin{align*}
& \mathbf{a^*} = (a_1^*, a_2^* \dots a_N^*) \\
& a_i^* = argmax_{a_i} Q_i(s, a_i), a_i \in A_i
\end{align*}
The value function used in Eq.\ref{eq:update} is:
\begin{equation}
V(s) = max_{a_i} Q_i(s, a_i) = Q_i(s, a_i^*)
\end{equation}

\subsection{Friend-Q}
In Friend-Q, each agent maintain a copy of Q-table of size $nS\times nA_1 \times\dots nA_N$. Each agent assumes other agents will corporate to maximize the Q values, and the greedy action is defined as:
\begin{align*}
& \mathbf{a^*} = (a_1^*, a_2^* \dots a_N^*) \\
& a_i^* = argmax_{\mathbf{a}} Q_i(s, \mathbf{a})[i], \mathbf{a} \in \mathbf{A}
\end{align*}
The value function is defined as:
\begin{equation}
V(s) = max_{\mathbf{a} } Q_i(s,  \mathbf{a})  = Q_i(s, \mathbf{a^*})
\end{equation}

\subsection{Foe-Q}
In Foe-Q learning, each agent maintain a of Q-table of size $nS \times nA_{opponent} \times nA_{self}$. The agents can follow a mixed policy: each agent $i$ takes action $a_i$ with probability $\sigma_i(a_i)$. The greedy policy is obtained from solving the minimax problem of $\sigma$:
\begin{equation}
\sigma_i^*(a) = max_{\sigma_i} min_{a_o\in A_{opponent}} \sum_{a_i\in A_{self}}Q_i(a_o, a_i) \sigma_i(a_i)
\end{equation}
and the greedy action can be sampled from the minimax probability: $a_i^* \sim \sigma_i^*(a)$.

Let the value function $V_i = min_{a_o} \sum_{a_i}Q_i(a_o, a_i) \sigma_i(a_i)$, the minimax equation can be formulated in the following linear programing problem:
\begin{align}
& \text{maximize $V_i$} \notag \\
%[-1, 0... 0] 
%\begin{bmatrix}
%V_i  \\
%\sigma_i
%\end{bmatrix} \notag \\
1.\; & \text{$nA_o$ inequality constraint:} \notag \notag\\
& V_i-\sum_{a_i\in A_i} Q_i(a_o, a_i)\sigma_i(a_i)\le 0, \forall a_o \in A_{opponent} \notag \\ 
%& [\mathbf{1}, -Q_i]
%\begin{bmatrix}
%V_i  \\
%\sigma_i
%\end{bmatrix}
%\le 0 \notag \\
2.\; & \text{$nA_i$ inequality constraint}:  \sigma_i(a_i) \ge 0, \forall a_i\in A_i \notag\\
%[\mathbf{0}, -I]  
%\begin{bmatrix}
%V_i  \\
%\sigma_i
%\end{bmatrix}
%\le 0 \notag \\
3.\; & \text{1 equality constraint:} \sum_{a_i\in A_i} \sigma_i(a_i) = 1
%& [0, 1\dots1]  
%\begin{bmatrix}
%V_i  \\
%\sigma_i
%\end{bmatrix}
%= 1
\end{align}
The second inequality constraint and equality constraint are simply probability restrictions. Foe-Q follows Alg.\ref{algo:foe-Q} to solve the minimax problem and update Q values. Note we first update $Q(s)$ with $V(s')$ then solve for $V(s)$ with the updated $Q(s)$, alternatively we can solve $V(s')$ from $Q(s')$ first, then update $Q(s)$. I believe the order shouldn't matter -- more like a chicken or egg problem. But with the first way we can initialize $V=0$ and no need to treat terminal state specially, the second way we don't need to save $V$ in memory.

\begin{algorithm}[h!]
	\caption{foe-Q update}
	\begin{algorithmic}
		\Function{foe-Q.update}{s, s', $\mathbf{a}$, $\mathbf{R}$}
		\State $Q_1(s, a_2, a_1) = (1-\alpha) Q_1(s, a_2, a_1) + \alpha  \big( (1-\gamma) R_1 + \gamma V_1(s') \big)$
		\State $Q_2(s, a_1, a_2) = (1-\alpha) Q_2(s, a_1, a_2) + \alpha  \big( (1-\gamma) R_2 + \gamma V_2(s') \big)$
		\State $V_1(s), \sigma_1(s) = minimax(Q_1(s))$
		\State $V_2(s), \sigma_2(s) = minimax(Q_2(s))$
		\EndFunction
	\end{algorithmic}
	\label{algo:foe-Q}
\end{algorithm}

\subsection{Correlated-Q}
In CE-Q, each agent maintain a copy of Q-table of size $nS\times nA_1 \times\dots nA_N$. All agents follows a mixed policy based on join probability $\sigma(\mathbf{a}) = \sigma(a_1, a_2 \dots a_N)$. The joint probability has to satisfy rational constraint as well as regular probability constraint. For utilitarian CE-Q, the objective function is to maximize the sum of the agentsâ€™ rewards, and the $u$CE-Q can be formulated in the following linear programing problem:

\begin{align}
&\text{maximize} \sum_i^{n_{agent}} \sum_{\mathbf{a}\in\mathbf{A}} Q_i(\mathbf{a}) \sigma(\mathbf{a}) \notag \\
1.\; & \text{Each agent contribute $nA_i(nA_i-1)$ rational constraint:} \notag\\
& \sum_{\mathbf{a_o}} Q_i(\mathbf{a_o}, a_i')\sigma(\mathbf{a_o}, a_i) \le \sum_{\mathbf{a_o}}  Q_i(\mathbf{a_o}, a_i)  \sigma(\mathbf{a_o}, a_i) \notag \\
& \forall a_i\neq a_i' \in A_i  \notag \\
& \mathbf{a_o} \in \{ A_1, A_2 \dots A_{i-1}, A_{i+1} \dots A_{N} \} \notag \\
2.\; & \text{$\Pi_i  nA_i$ inequality constraint}: \sigma(\mathbf{a})\le 0,  \forall \mathbf{a}\in\mathbf{A}\notag \\
3.\; &\text{1 equality constraint:} \sum_{\mathbf{a}\in\mathbf{A}} \sigma(\mathbf{a}) = 1
\end{align}
The rational probability can be interpret as following: assuming there is a referee recommend all agent to act according to joint probability distribution $\sigma(\mathbf{a})$. Assuming all other agent follows the referee's advise, for agent $i$, the expected reward for playing the recommended action $a_i$ is $\sum_{\mathbf{a_o}}  Q_i(\mathbf{a_o}, a_i)  \sigma(\mathbf{a_o}, a_i)$, while for playing another action $a_i'$ $\sum_{\mathbf{a_o}}  Q_i(\mathbf{a_o}, a_i')  \sigma(\mathbf{a_o}, a_i)$. The rational constraint simply says, at correlated equilibrium, an agent receives maximum reward if he follows referee's recommendation, given other agents also follow the referee.

After solving the above LP and obtain the joint probability, the state value function can be calculated as: $V_i(s) = \sum_{\mathbf{a}} Q_i(s, \mathbf{a}) \sigma(s, \mathbf{a})$. The $u$CE-Q updating procedure is summarized in Alg.\ref{algo:ce-Q}.

\begin{algorithm}[h!]
	\caption{Correlated-Q update}
	\begin{algorithmic}
		\Function{CE-Q.update}{s, s', $\mathbf{a}$, $\mathbf{R}$}
		\For {i = 1,2$\dots$ $n_{agent}$}
			\State $Q_i(s, \mathbf{a}) = (1-\alpha) Q_i(s, \mathbf{a}) + \alpha  \big( (1-\gamma) R_i + \gamma V_i(s') \big)$
		\EndFor
		\State $\sigma(s, \mathbf{a}) = uCE(\{Q_1(s), Q_2(s)\dots Q_N(s)\})$ 
		\For {i = 1,2$\dots$ $n_{agent}$}
		\State $V_i(s) = \sum_{\mathbf{a}} Q_i(s, \mathbf{a}) \sigma(s, \mathbf{a})$
		\EndFor
		\EndFunction
	\end{algorithmic}
	\label{algo:ce-Q}
\end{algorithm}

\section{Experiment Results}
While training all learners, we keep track of the Q value of state $s$ (as show in Fig.\ref{fig:state_s}), with player A taking action S and player B sticking to reproduce figure.3 in Greenward 2003 paper. Since in state $s$, taking action N would bump into the wall and result in stick effectively, we will add up the Q value of player B taking N and stick together. It is unclear to me whether the iteration in x-axis means each time step or each episode in Greenward's paper, but given the large number I would assume it is every time step -- although this doesn't make material difference that affect our conclusion. 

We decay learning rate $\alpha$ from 1 to 0.001 with exponential decay rate 0.99999. For Q-Learning, we use $\epsilon$-greedy with $\epsilon$ decaying from 1 to 0.001, exponential decay rate 0.99999. For Friend-Q, Foe-Q and CE-Q, use random exploration for choosing actions ($\epsilon=1$). All learner are trained for 1,000,000 iterations. The discount factor $\gamma=0.9$.

Q-Learning does not converge. In Fig.\ref{fig:results}(a), we can see the difference in Q-value decreases at the beginning, but this is the decaying of learning rate. When the learning rate reduce to minimum (0.001), we can see there is constant fluctuation of the error. Q-Learning tries to find a pure policy, while for this game, the equilibrium is a mixed policy.

From Fig.\ref{fig:results}(b), we can see the Q values converges quickly for friend-Q. However, the resulting actions are not rational. From the Q-table of state $s$, we can see for player B, action $E$ dominate other actions, so player B would move right and expect player A to move aside or score for him. The best Q-value for player B is both A and B take action $E$, because there is 50\% chance $B$ can score +100 in the current round. The Q-table for player A has $Q_A[:, W]=10$, this means A can take any action and expect B to move $W$ and score for him. Note the maximum possible Q-value $\pm10$ results from the $(1-\gamma)R_i$ term when we have $\gamma=0.9$.

Foe-Q and $u$CE-Q also converges. If the initialization are the same, the learning curves are nearly identical for both learners. The final Q-value difference are with in $10^{-7}$ and both learner gives a mixed policy with both player randomly choose between stick and heading south: $\sigma_A(stick/N)=0.525, \sigma_A(S)=0.475, \sigma_B(stick/N)=0.455, \sigma_B(S)=0.545$. Note for $u$CE-Q each players policy is  marginal probability computed from joint probability $\sigma_i(a_i) = \sum_{\mathbf{a_o}}\sigma(\mathbf{a_o}, a_i)$.

We also experimented with the learning rate decaying schedule in Greenward's 2005 paper, $\alpha(s,\mathbf{a}) = 1/n(s,\mathbf{a})$ and observed similar behavior for all learners. However, the final policy of foe-Q and CE-Q is a bit different from that of exponential decay, $\sigma_A(stick/N)=0.4, \sigma_A(S)=0.6, \sigma_B(stick/N)=0.615, \sigma_B(S)=0.385$. It is possible that one or both schemes haven't converged yet. I verified that when passing a same Q table, the Foe-Q minimax solver and the $u$CE-Q solver would give the same policy and state value -- so I am a bit more confident with my implementation, the difference in final results is likely due to training hyperparameters.

The final Q-table of state $s$ for both Foe-Q and CE-Q is shown in Fig.\ref{fig:foe-ce-Q}. Interestingly, in Foe-Q and CE-Q, two players Q-table preserve the property $Q_A = -Q_B$ during training (If not initialized this way, it empirically converge to something close). For Friend-Q, the converged Q-table does not satisfy $Q_A = -Q_B$ (also see Fig.\ref{fig:friendQ}). As far as I understand, in zero-sum game, there isn't a collaborative equilibrium, so friend-Q does not necessarily converge to Nash equilibrium. Foe-Q converge to Nash equilibrium because there is adversarial equilibrium, and the collaborative equilibrium coincide with Nash equilibrium in case of zero-sum game. Also, in Friend-Q, the Q-table reflects the values when each player expect the other being collaborative, this is not the case in reality however. While in Foe-Q and CE-Q, the other player is behaving as expected --- playing minimax or follow the ``referee".

\begin{figure}[h!]
	\centering
	\includegraphics[width=3.4in]{figures/friendQ_A.png}\\
	(a) player A\\
	\includegraphics[width=3.4in]{figures/friendQ_B.png}\\
	(b) player B
	\caption{Q table of state $s$ of trained Friend-Q learner}
	\label{fig:friendQ}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=3.4in]{figures/foeQ_A.png}\\
	(a) player A\\
	\includegraphics[width=3.4in]{figures/foeQ_B.png}\\
	(b) player B
	\caption{Q table of state $s$ of trained Foe-Q or CE-Q learner}
	\label{fig:foe-ce-Q}
\end{figure}


\begin{figure*}[h!]
	\begin{tabular}{cc}
		\includegraphics[width=3.4in]{figures/Q.png} &
		\includegraphics[width=3.4in]{figures/friendQ.png} \\
		(a) Q-Learning & (b) Friend-Q \\
		\includegraphics[width=3.4in]{figures/foeQ.png} &
		\includegraphics[width=3.4in]{figures/ceQ.png} \\
		(a) Foe-Q & (b) $u$CE-Q \\
	\end{tabular}
	\caption{The difference of player A's Q value for state $s$ and player A taking action S and player B sticking/N in each iteration, $|Q_A^t(s, S, Stick/N) - Q_A^{t-1}(s, S, Stick/N)|$ (the TD error in each iteration). $\alpha\rightarrow0.001$. $\gamma=0.9$, For Q-Learning, $\epsilon\rightarrow0.001$. For Friend-Q, Foe-Q, CE-Q, $\epsilon=1$. }
	\label{fig:results}
\end{figure*}

\section{Pitfalls}
There are several pitfalls in this project that requires future work.

First, Greenward's paper uses on-policy/SARSA update for Q-Learner, while we used off-policy so that everything can be fit into the general off-policy update template \ref{algo:learning}.

It would also be interesting to implement other variant of CE-Q learners. For $e$CE-Q and $r$CE-Q, we may need to introduce extra variable $u=\sum_{\mathbf{a}}\sigma(\mathbf{a}) Q_i(\mathbf{a})$, solving $max(u)$ with extra constraint $u>(or <) \sum_{\mathbf{a}}\sigma(\mathbf{a}) Q_i(\mathbf{a}), \forall i$. $l$CE-Q would require an decentralized algorithm that each agent solves one LP.

	
\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}
