\relax 
\citation{greenwald2003correlated}
\citation{greenwald2003correlated}
\citation{greenwald2003correlated}
\citation{littman2001friend}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Soccer game, state $s$ (Figure 4 or Greenward 2003 paper\cite  {greenwald2003correlated})}}{1}}
\newlabel{fig:state_s}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Soccer Game}{1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces soccer game environment implementation}}{1}}
\newlabel{algo:soccer}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Learner implementation}{1}}
\newlabel{eq:update}{{1}{1}}
\citation{greenwald2003correlated}
\citation{greenwald2005correlated}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces off\_policy\_learning template}}{2}}
\newlabel{algo:learning}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Q-Learning}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Friend-Q}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Foe-Q}{2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces foe-Q update}}{2}}
\newlabel{algo:foe-Q}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-D}}Correlated-Q}{2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Correlated-Q update}}{3}}
\newlabel{algo:ce-Q}{{4}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experiment Results}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Pitfalls}{3}}
\bibstyle{IEEEtran}
\bibdata{reference}
\bibcite{greenwald2003correlated}{1}
\bibcite{littman2001friend}{2}
\bibcite{greenwald2005correlated}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Q table of state $s$ of trained Friend-Q learner}}{4}}
\newlabel{fig:friendQ}{{2}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Q table of state $s$ of trained Foe-Q or CE-Q learner}}{4}}
\newlabel{fig:foe-ce-Q}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The difference of player A's Q value for state $s$ and player A taking action S and player B sticking/N in each iteration, $|Q_A^t(s, S, Stick/N) - Q_A^{t-1}(s, S, Stick/N)|$ (the TD error). For Q-Learning, $\alpha \rightarrow 0.001$, $\epsilon \rightarrow 0.001$ with decay rate 0.99995. For Friend-Q, Foe-Q, CE-Q, $\alpha (s,\mathbf  {a})=1/n(s,\mathbf  {a})$, $\epsilon =1$. $\gamma =0.9$}}{5}}
\newlabel{fig:results}{{4}{5}}
