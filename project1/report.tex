\documentclass{article}
%\documentclass[aip,jcp,amsmath,amssymb,reprint]{revtex4-1}
\usepackage[utf8]{inputenc}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\minuseq}{\mathrel{-}=}
\newcommand{\asteq}{\mathrel{*}=}
\renewcommand{\algorithmiccomment}[1]{\hfill$\triangleright$\textit{#1}}

\title{CS7642 Project 1 report}
\author{Qinghui Ge}
\date{January 2020}

\begin{document}

\maketitle

\section{Problem}

\section{Implementation}
Same as in Sutton's paper, I prepare 100 training sets, each containing 10 sequences of random walk. The pseudo-code for generating sequence is presented in Algorithm~\ref{algo:seq}. Although the sequences can be generated on-the-fly during training process, I prepare all training sets in the beginning, and use the same 100 training sets through all experiments. This is consistent with Sutton's setting.


\begin{algorithm}[h!]
\caption{Prepare Training Sets}
\begin{algorithmic}
\Function{generate\_sequence}{start, n\_state} 
    \State seq = [start]
    \State s = n\_state
    \While{s != 0 and s!=n\_state-1}
        \State $s \pluseq (rand()<0.5?\; 1: -1)$
        \State seq.append(s)
    \EndWhile
    \State return seq
\EndFunction

\Comment Generate 100 training sets, each has 10 sequences. Each sequence starts at state D(idx=3) and ends at A(idx=0) or G(idx=6)

\For {i in 1,2,...100}
    \For {j in 1,2,...10}
        \State all\_training\_set[i][j] = GENERATE\_SEQUENCE(start=3, n\_state=7)
    \EndFor
\EndFor

\end{algorithmic}
\label{algo:seq}
\end{algorithm}

I explored two slightly different implementations of Eq.1 (described in Algorithm.~\ref{algo:dw1} and ~\ref{algo:dw2}). In each time step, we compute $\Delta \omega_t = \alpha * (P_{t+1}-P_t) * e_t$, where $\alpha$ is the learning rate, $P_t = \omega^T x_t$. $e_t$ is updated by scaling the previous $e_{t-1}$ by the factor $\lambda$, and adding $\Delta e_t = \nabla_\omega P_t = x_t$. The $x's$ are vectors representing each state. As in Sutton's paper, the simplest choice of $x$ are unit vectors, in which case $P_t$ is $seq[t]^{th}$ element of $\omega$. Any other set of orthonormal basis vectors can be used to represent the states, but the exact forms of $P_t$ and $\Delta e_t$ have to be changed accordingly, and the $\omega$ values can not be directly interpreted at the expectation value of $z$ (has to take the general form $\omega^T x$).

The two algorithms differ in whether the weights of the terminate states $A$ and $G$ are trainable. In Algorithm~\ref{algo:dw1}, the first and last elements of $\omega$ vector are initialized to $z$ value of corresponding terminate states (0 and 1) and never get updated in the time-iteration. In Algorithm~\ref{algo:dw2}, the two elements are trainable, they get updated in the last time step when the terminate state is reached. These two elements can be initialized to 0.5 as other elements, and will approach 0 and 1 respectively given the training converges. The two algorithms gives similar results and I decided to stick with Algorithm~\ref{algo:dw1}, because it is slightly simpler and more efficient (as final signal needs to propagate through 1 less state). As Sutton didn't consider the error of first and last elements, and his state vectors only have 5 elements, I think Algorithm~\ref{algo:dw1} is also what Sutton used.

\begin{algorithm}[h!]
\caption{compute $\Delta\omega$ given a sequence}
\begin{algorithmic}
\Function{update}{$\omega$, seq, $\alpha$, $\lambda$} 
    \Statex \Comment {first and last element of $\omega$ should be initialized to the true observation value z (0 and 1 in this case), they will not be updated during the iteration.}
    \State $n_s$ = len($\omega$) 
    \State initialize $e_t = [0]*n_s$ 
    \State initialize $\Delta\omega = [0]*n_s$ 
    \For {t = 0,1,2...len(seq)-2}
    \State     $\Delta e = [0]*n_s$ 
    \State     $\Delta e[\mathrm{seq}[t]] = 1$ 
    \State     $e_t = \Delta e + \lambda e_t$ 
    \State     $P_t = \omega[\mathrm{seq}[t]]$ 
    \State     $P_{t+1} = \omega[\mathrm{seq}[t+1]]$ 
    \State     $\Delta \omega_t = \alpha * (P_{t+1} - P_t) * e_t$ 
    \State     $\Delta \omega \pluseq \Delta \omega_t $
    \EndFor
\EndFunction
\end{algorithmic}
\label{algo:dw1}
\end{algorithm}

\begin{algorithm}[h!]
\caption{Alternative way of compute $\Delta\omega$ given a sequence}
\begin{algorithmic}
\Function{update}{$\omega$, seq, $\alpha$, $\lambda$} 
    \Statex \Comment {In this case, first and last element of $\omega$ can be initialized like other elements and will be updated in the iteration.}
    \State $n_s$ = len($\omega$) 
    \State initialize $e_t = [0]*n_s$ 
    \State initialize $\Delta\omega = [0]*n_s$ 
    \For {t = 0,1,2...len(seq)-1}
    \State $\Delta e = [0]*n_s$ 
    \State $\Delta e[\mathrm{seq}[t]] = 1$ 
    \State $e_t = \Delta e + \lambda e_t$ 
    \State $P_t = \omega[\mathrm{seq}[t]]$ 
        \If {t!=len(seq)-1}
            \State $P_{t+1} = \omega[\mathrm{seq}[t+1]]$ 
        \Else:
            \State $P_{t+1}$ = (seq[t]==0? 0: 1)
        \EndIf
    \State     $\Delta \omega_t = \alpha * (P_{t+1} - P_t) * e_t$ 
    \State     $\Delta \omega \pluseq \Delta \omega_t $
    \EndFor
\EndFunction
\end{algorithmic}
\label{algo:dw2}
\end{algorithm}

Sutton's two experiments follows different updating procedure given each training set (10 sequences). Algorithm~\ref{algo:ex1} shows the pseudo-code for experiment 1. The $\omega$ values are updated after seeing the entire training set, during which $\Delta \omega$ is accumulated. Each training set is presented multiple times until $\omega$ converges. In my implementation, the convergence criterion is set to be ... Smaller thresholds are experimented and did not bring significant change in results. It is worth noting that as $\Delta \omega$ is accumulated for the entire training set, experiment 1 typically needs a smaller learning rate than experiment 2. Sutton states in his paper that a small enough learning rate can ensure the convergence (but will certainly requires more iteration). In my implementation, I choose $\alpha=0.1$ and scale it by the size of training set. All training is able to converge under this effective learning rate (0.01), without reaching maximum iteration = 10000.


\begin{algorithm}[h!]
\caption{Experiment 1 $\omega$ update on single training set}
\begin{algorithmic}
\Function{training\_1}{$\alpha$, $\lambda$, training\_set} 
    \State $\omega$ = [0, 0,5, 0.5, 0.5, 0.5, 0.5, 1]
    \For {n = 1,2,3...max\_iteration}
        \For {seq in training\_set}
            \State $\Delta \omega \pluseq \mathrm{UPDATE}(\omega, \mathrm{seq}, \alpha/\mathrm{traing\_set\_size}, \lambda)$
        \EndFor
        \State $\omega \pluseq \Delta \omega$
        \If {sum(abs($\Delta \omega$)) $<$ thresh}
            \State break
        \EndIf
    \EndFor
    
    \State return RMSE($\omega$[1:-1], $\omega_{true}$)
\EndFunction
\end{algorithmic}
\label{algo:ex1}
\end{algorithm}

In experiment 2 (see Algorithm~\ref{algo:ex1}), each training set is only presented once, and $\omega$ vector is updated for each sequence of a training set.

\begin{algorithm}[h!]
\caption{Experiment 2 $\omega$ update on single training set}
\begin{algorithmic}
\Function{training\_2}{$\alpha$, $\lambda$, training\_set} 
    \State $\omega$ = [0, 0,5, 0.5, 0.5, 0.5, 0.5, 1]
    \For {seq in training\_set}
        \State $\omega \pluseq \mathrm{UPDATE}(\omega, \mathrm{seq}, \alpha, \lambda)$
    \EndFor
    
    \State return RMSE($\omega$[1:-1], $\omega_{true}$)
\EndFunction
\end{algorithmic}
\label{algo:ex2}
\end{algorithm}

Both Algorithm~\ref{algo:ex1} and Algorithm~\ref{algo:ex2} return the root-mean-squared error (RMSE) between $\omega$ and the theoretical values ([$\frac{1}{6},\frac{1}{3},\frac{1}{2},\frac{2}{3},\frac{5}{6}$]) after training on a given training set. Finally, given a parameter set \{$\alpha$ and $\lambda$\}, both experiment 1 and 2 use Algorithm~\ref{algo:final} to compute the average and standard deviation of RMSE in 100 training set.

\begin{algorithm}[h!]
\caption{Compute the average and standard deviation of RMSE error in all training set}
\begin{algorithmic}
\Function{compute\_error}{$\alpha$, $\lambda$, all\_training\_set} 
    \State errors = []
    \For {training\_set in all\_training\_set}
        \State rmse = TRAINING\_1/2($\alpha$, $\lambda$, training\_set)
        \State errors.append(rmse)
    \EndFor
    
    \State return mean(errors), std(errors)
\EndFunction
\end{algorithmic}
\label{algo:final}
\end{algorithm}

\section{Results}
Before trying to reproduce Sutton's results, I did a quick sanity check on my UPDATE algorithm: I averaged the $\omega$ of 1000 training set with size 100. The average $\omega$ vector is [0.162, 0.331, 0.495, 0.666, 0.835, 1], which is very close to theoretical value, with RMSE=0.00337.

I reproduced figure 3,4,5 in Sutton's paper. In addition to average RMSE, I also plot the standard error (estimated by $SE=\frac{std(RMSE's)}{\sqrt{n\_training\_set}}=\frac{std(RMSE's)}{10}$) in order to have a sense of statistical significance. 

Figure 3 plots the average RMSE vs different choices of $\lambda$. Both my and Sutton's figure show the optimal $\lambda$ is $\lambda=0$, and the error increases as $\lambda$ increases toward 1 (Widrow-Hoff limit). Despite agreeing on general trend, my RMSE ranges from around 0.11-0.18, while Sutton's ranges from 0.19-0.25. Initially I suspect this is due to randomness of training sets preparation, however, this is unlikely given the standard errors $SE<0.01$ (this is consistent with Sutton, who also reports $SE<0.01$). In addition, a few different run changing random seed do not alter my results significantly.

Figure 4 shows the average RMSE while altering learning rate $\alpha$, at four different $\lambda$ values (0,0.3,0.7,1). When $\alpha=0$, there is no update on $\omega$, so the error for $\lambda's$ is the RMSE of the initial guess ([0.5,0.5,0.5,0.5,0.5]), which is 0.2357. The general shape of the lines are similar to Sutton's. Errors of $\lambda=1$ increase quickly as $\lambda$ increases. Errors of $\lambda = 0.3$ and $0.8$ are relatively insensitive to $\alpha$. The $\alpha$'s that give minimum error for the above 4 $\lambda$ values are (0.2, 0.2, 0.1, 0.05), while in Sutton's paper there were (0.3, 0.3, 0.15, 0.05). The standard error for the larger $\alpha$ values are quite large, so the lines in this region can change significantly between different runs. As I understand the last sequence in each training set would have stronger influence on $\omega$, especially when $\alpha$ is large, causing more variance between training sets.

Figure 5 shows the smallest error for each $\lambda$ with optimized learning rate $\alpha$. $\lambda=0.2$ has smallest error. In Sutton's paper, the minimum falls between 0.2-0.3. The minimal error is around 0.09, while Sutton's minimum is above 0.1. In both my and Sutton's results, $\lambda=1$ gives largest error, but maximum value is different. Overall, the results shares similar pattern as in Sutton's paper, with noticeable difference in the range of RMSE values. Again, the small standard error (especially for smaller $\lambda$'s) makes me think that the discrepancy is unlikely due to randomness in training sets.

\end{document}
